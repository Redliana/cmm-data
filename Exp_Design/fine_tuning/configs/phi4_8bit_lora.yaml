# Phi-4 LoRA fine-tuning config (8-bit quantized)
# More memory headroom -> larger batch size
model: mlx-community/phi-4-8bit
data: data
adapter_path: adapters/phi4_8bit_lora

# LoRA hyperparameters
lora_layers: 16
lora_parameters:
  rank: 16
  alpha: 32.0
  dropout: 0.05
  scale: 2.0

# Training
batch_size: 8
iters: 1000
val_batches: 25
steps_per_report: 10
steps_per_eval: 100
save_every: 200

# Optimizer
learning_rate: 1.0e-5
lr_schedule:
  name: cosine_decay
  warmup: 100
  arguments: [1.0e-5, 1000, 1.0e-7]

# Memory optimization
grad_checkpoint: true
max_seq_length: 2048

seed: 42
